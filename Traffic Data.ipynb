{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXs-qEUa77sv"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from shapely import Point\n",
        "import numpy as np\n",
        "\n",
        "# Load the NHGIS data table\n",
        "df_table = pd.read_csv(\n",
        "    r\"nhgis0004_ds267_20235_tract.csv\",\n",
        "    low_memory=False\n",
        ")\n",
        "\n",
        "# Set all negative values to 0 in mediam household income (this became an issue much later on)\n",
        "df_table['ASQPE001'] = df_table['ASQPE001'].where(df_table['ASQPE001'] >= 0, 0)\n",
        "\n",
        "# Remove the low_memory=False parameter and try with default settings\n",
        "gis_shap = gpd.read_file(r\"US_tract_2023.shp\")\n",
        "\n",
        "# Immediately filter to Michigan to reduce memory usage\n",
        "gis_merged = gis_shap.merge(df_table, on='GISJOIN')\n",
        "gis_mi = gis_merged[gis_merged['STATE'] == 'Michigan'].copy()\n",
        "\n",
        "# Delete the large objects to free memory\n",
        "del gis_merged, gis_shap, df_table\n",
        "\n",
        "# Load SEMCOG dataset\n",
        "semcog = pd.read_csv(r\"crash2024_10year_-2940358597150127354.csv\", low_memory=False)\n",
        "semcog = semcog[semcog['YEAR'].isin([2023, 2022, 2021, 2020, 2019])]\n",
        "\n",
        "g = [Point(xy) for xy in zip(semcog['XCORD'], semcog['YCORD'])]\n",
        "\n",
        "# EPSG:4326 is the CRS I useed because it is the standard fot GCS system, which use the latitude/longitude sysetm present in the semcog data set\n",
        "semcog_gdf = gpd.GeoDataFrame(semcog, geometry=g, crs=\"EPSG:4326\")\n",
        "\n",
        "# here I set the coordinate control system (CRS) of the semcog data set to the CRS  of the gis_mi data\n",
        "semcog_gdf = semcog_gdf.to_crs(gis_mi.crs)\n",
        "\n",
        "# Spatial join to assign tracts, used predicate within because I only wanted the ones, which were, well... within\n",
        "semcog_new = gpd.sjoin(semcog_gdf, gis_mi, how=\"left\", predicate=\"within\")\n",
        "del g, semcog_gdf, gis_mi, semcog\n",
        "\n",
        "# Dropped all the columns related to Margin of Error to make data easier to intrepet and load\n",
        "moe_cols_to_drop = [\n",
        "    'NAME_M',\n",
        "    'ASOAM001', 'ASOAM002', 'ASOAM003', 'ASOAM004', 'ASOAM005', 'ASOAM006', 'ASOAM007', 'ASOAM008', 'ASOAM009', 'ASOAM010',\n",
        "    'ASOAM011', 'ASOAM012', 'ASOAM013', 'ASOAM014', 'ASOAM015', 'ASOAM016', 'ASOAM017', 'ASOAM018', 'ASOAM019', 'ASOAM020', 'ASOAM021',\n",
        "    'ASQIM001', 'ASQIM002', 'ASQIM003', 'ASQIM004', 'ASQIM005', 'ASQIM006', 'ASQIM007', 'ASQIM008',\n",
        "    'ASQOM001', 'ASQOM002', 'ASQOM003', 'ASQOM004', 'ASQOM005', 'ASQOM006', 'ASQOM007', 'ASQOM008', 'ASQOM009', 'ASQOM010',\n",
        "    'ASQOM011', 'ASQOM012', 'ASQOM013', 'ASQOM014', 'ASQOM015', 'ASQOM016', 'ASQOM017',\n",
        "    'ASQPM001'\n",
        "]\n",
        "\n",
        "semcog_new = semcog_new.drop(columns=moe_cols_to_drop, errors='ignore')\n",
        "del moe_cols_to_drop\n",
        "\n",
        "# I renamed all my columns for simplicity. The metadata included what each column stood for and thus I used definations from there\n",
        "def rename_columns(df):\n",
        "    rename_dict = {\n",
        "        # Hispanic or Latino Origin by Race (Estimates)\n",
        "        'ASOAE001': 'Total_Population',\n",
        "        'ASOAE002': 'Not_Hispanic_or_Latino',\n",
        "        'ASOAE003': 'White',\n",
        "        'ASOAE004': 'Black',\n",
        "        'ASOAE005': 'Not_Hispanic_AIAN_Alone',\n",
        "        'ASOAE006': 'Asian',\n",
        "        'ASOAE007': 'NHPI',\n",
        "        'ASOAE008': 'Not_Hispanic_Other_Race_Alone',\n",
        "        'ASOAE009': 'Not_Hispanic_Two_or_More_Races',\n",
        "        'ASOAE010': 'Not_Hispanic_Two_Races_SomeOther',\n",
        "        'ASOAE011': 'Not_Hispanic_Two_Races_Exclude_SomeOther',\n",
        "        'ASOAE012': 'Hispanic_or_Latino',\n",
        "        'ASOAE013': 'Hispanic_White',\n",
        "        # Keep individual columns for now - we'll combine them after renaming\n",
        "        'ASOAE014': 'Hispanic_Black_Alone',\n",
        "        'ASOAE015': 'Hispanic_AIAN_Alone',\n",
        "        'ASOAE016': 'Hispanic_Asian_Alone',\n",
        "        'ASOAE017': 'Hispanic_NHPI_Alone',\n",
        "        'ASOAE018': 'Hispanic_Other_Race_Alone',\n",
        "        'ASOAE019': 'Hispanic_Two_or_More_Races',\n",
        "        'ASOAE020': 'Hispanic_Two_Races_SomeOther',\n",
        "        'ASOAE021': 'Hispanic_Two_Races_Exclude_SomeOther',\n",
        "        # Ratio of Income to Poverty Level\n",
        "        'ASQIE001': 'Poverty_Total',\n",
        "        'ASQIE002': 'Under_50_Percent_Poverty',\n",
        "        'ASQIE003': '50_to_99_Percent_Poverty',\n",
        "        'ASQIE004': '100_to_124_Percent_Poverty',\n",
        "        'ASQIE005': '125_to_149_Percent_Poverty',\n",
        "        'ASQIE006': '150_to_184_Percent_Poverty',\n",
        "        'ASQIE007': '185_to_199_Percent_Poverty',\n",
        "        'ASQIE008': '200_Percent_or_More_Poverty',\n",
        "        # Household (HH) Income Ranges\n",
        "        'ASQOE001': 'HH_Income_Total',\n",
        "        'ASQOE002': 'HH_Income_Less_Than_10K',\n",
        "        'ASQOE003': 'HH_Income_10K_to_14_999',\n",
        "        'ASQOE004': 'HH_Income_15K_to_19_999',\n",
        "        'ASQOE005': 'HH_Income_20K_to_24_999',\n",
        "        'ASQOE006': 'HH_Income_25K_to_29_999',\n",
        "        'ASQOE007': 'HH_Income_30K_to_34_999',\n",
        "        'ASQOE008': 'HH_Income_35K_to_39_999',\n",
        "        'ASQOE009': 'HH_Income_40K_to_44_999',\n",
        "        'ASQOE010': 'HH_Income_45K_to_49_999',\n",
        "        'ASQOE011': 'HH_Income_50K_to_59_999',\n",
        "        'ASQOE012': 'HH_Income_60K_to_74_999',\n",
        "        'ASQOE013': 'HH_Income_75K_to_99_999',\n",
        "        'ASQIE014': 'HH_Income_100K_to_124_999',\n",
        "        'ASQOE015': 'HH_Income_125K_to_149_999',\n",
        "        'ASQOE016': 'HH_Income_150K_to_199_999',\n",
        "        'ASQOE017': 'HH_Income_200K_or_More',\n",
        "        # Median Household Income\n",
        "        'ASQPE001': 'Median_HH_Income_2023'\n",
        "    }\n",
        "\n",
        "    return df.rename(columns=rename_dict)\n",
        "\n",
        "# Running rename fucntion above\n",
        "semcog_new = rename_columns(semcog_new)\n",
        "\n",
        "# Combine all hispanic non-white columns\n",
        "hispanic_non_white_cols = [\n",
        "    'Hispanic_Black_Alone',\n",
        "    'Hispanic_AIAN_Alone',\n",
        "    'Hispanic_Asian_Alone',\n",
        "    'Hispanic_NHPI_Alone',\n",
        "    'Hispanic_Other_Race_Alone',\n",
        "    'Hispanic_Two_or_More_Races',\n",
        "    'Hispanic_Two_Races_SomeOther',\n",
        "    'Hispanic_Two_Races_Exclude_SomeOther'\n",
        "]\n",
        "existing_hispanic_cols = [c for c in hispanic_non_white_cols if c in semcog_new.columns]\n",
        "\n",
        "if existing_hispanic_cols:\n",
        "    semcog_new['Hispanic_non_White'] = (\n",
        "        semcog_new[existing_hispanic_cols].fillna(0).sum(axis=1)\n",
        "    )\n",
        "else:\n",
        "    semcog_new['Hispanic_non_White'] = 0\n",
        "# Sum the Hispanic non-White columns (fill NaN with 0 first to handle missing values)\n",
        "semcog_new['Hispanic_non_White'] = semcog_new[hispanic_non_white_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "# Create semcog_new_2 with percentage calculations\n",
        "semcog_new_2 = semcog_new.copy()\n",
        "\n",
        "del hispanic_non_white_cols\n",
        "\n",
        "# Calculate percentages for all demographic variables (divide by Total_Population)\n",
        "demographic_cols = [\n",
        "    'Total_Population',\n",
        "    'Not_Hispanic_or_Latino',\n",
        "    'Not_Hispanic_White_Alone',\n",
        "    'Not_Hispanic_Black_Alone',\n",
        "    'Not_Hispanic_AIAN_Alone',\n",
        "    'Not_Hispanic_Asian_Alone',\n",
        "    'Not_Hispanic_NHPI_Alone',\n",
        "    'Not_Hispanic_Other_Race_Alone',\n",
        "    'Not_Hispanic_Two_or_More_Races',\n",
        "    'Not_Hispanic_Two_Races_SomeOther',\n",
        "    'Not_Hispanic_Two_Races_Exclude_SomeOther',\n",
        "    'Hispanic_or_Latino',\n",
        "    'Hispanic_White',\n",
        "    'Hispanic_non_White'\n",
        "]\n",
        "\n",
        "# Calculate demographic percentages\n",
        "for col in demographic_cols:\n",
        "    if col in semcog_new_2.columns:\n",
        "        semcog_new_2[f'{col}_Percent_values'] = (semcog_new_2[col] / semcog_new_2['Total_Population']) * 100\n",
        "del demographic_cols\n",
        "\n",
        "# Calculate percentages for poverty variables (divide by Poverty_Total)\n",
        "poverty_cols = [\n",
        "    'Under_50_Percent_Poverty',\n",
        "    '50_to_99_Percent_Poverty',\n",
        "    '100_to_124_Percent_Poverty',\n",
        "    '125_to_149_Percent_Poverty',\n",
        "    '150_to_184_Percent_Poverty',\n",
        "    '185_to_199_Percent_Poverty',\n",
        "    '200_Percent_or_More_Poverty'\n",
        "]\n",
        "\n",
        "# Calculate poverty percentages\n",
        "for col in poverty_cols:\n",
        "    if col in semcog_new_2.columns:\n",
        "        semcog_new_2[f'{col}_Percent_values'] = (semcog_new_2[col] / semcog_new_2['Poverty_Total']) * 100\n",
        "del poverty_cols\n",
        "\n",
        "# Calculate percentages for household income variables (divide by HH_Income_Total)\n",
        "hh_income_cols = [\n",
        "    'HH_Income_Less_Than_10K',\n",
        "    'HH_Income_10K_to_14_999',\n",
        "    'HH_Income_15K_to_19_999',\n",
        "    'HH_Income_20K_to_24_999',\n",
        "    'HH_Income_25K_to_29_999',\n",
        "    'HH_Income_30K_to_34_999',\n",
        "    'HH_Income_35K_to_39_999',\n",
        "    'HH_Income_40K_to_44_999',\n",
        "    'HH_Income_45K_to_49_999',\n",
        "    'HH_Income_50K_to_59_999',\n",
        "    'HH_Income_60K_to_74_999',\n",
        "    'HH_Income_75K_to_99_999',\n",
        "    'HH_Income_100K_to_124_999',\n",
        "    'HH_Income_125K_to_149_999',\n",
        "    'HH_Income_150K_to_199_999',\n",
        "    'HH_Income_200K_or_More'\n",
        "]\n",
        "\n",
        "# Calculate household income percentages\n",
        "for col in hh_income_cols:\n",
        "    if col in semcog_new_2.columns:\n",
        "        semcog_new_2[f'{col}_Percent_values'] = (semcog_new_2[col] / semcog_new_2['HH_Income_Total']) * 100\n",
        "del hh_income_cols\n",
        "# Handle division by replacing NaN values with 0\n",
        "semcog_new_2 = semcog_new_2.fillna(0)\n",
        "\n",
        "# Because now thhey are over 200 columns I decided to drop the oroginal colums adn only keep percent ones\n",
        "moe_cols_to_drop_2 = [\n",
        "    'HH_Income_Less_Than_10K',\n",
        "    'HH_Income_10K_to_14_999',\n",
        "    'HH_Income_15K_to_19_999',\n",
        "    'HH_Income_20K_to_24_999',\n",
        "    'HH_Income_25K_to_29_999',\n",
        "    'HH_Income_40K_to_44_999',\n",
        "    'HH_Income_30K_to_34_999',\n",
        "    'HH_Income_35K_to_39_999',\n",
        "    'HH_Income_45K_to_49_999',\n",
        "    'HH_Income_50K_to_59_999',\n",
        "    'HH_Income_60K_to_74_999',\n",
        "    'HH_Income_75K_to_99_999',\n",
        "    'HH_Income_100K_to_124_999',\n",
        "    'HH_Income_125K_to_149_999',\n",
        "    'HH_Income_150K_to_199_999',\n",
        "    'HH_Income_200K_or_More',\n",
        "    'Under_50_Percent_Poverty',\n",
        "    '50_to_99_Percent_Poverty',\n",
        "    '100_to_124_Percent_Poverty',\n",
        "    '125_to_149_Percent_Poverty',\n",
        "    '150_to_184_Percent_Poverty',\n",
        "    '185_to_199_Percent_Poverty',\n",
        "    '200_Percent_or_More_Poverty',\n",
        "    'Total_Population',\n",
        "    'Not_Hispanic_or_Latino',\n",
        "    'Not_Hispanic_White_Alone',\n",
        "    'Not_Hispanic_Black_Alone',\n",
        "    'Not_Hispanic_AIAN_Alone',\n",
        "    'Not_Hispanic_Asian_Alone',\n",
        "    'Not_Hispanic_NHPI_Alone',\n",
        "    'Not_Hispanic_Other_Race_Alone',\n",
        "    'Not_Hispanic_Two_or_More_Races',\n",
        "    'Not_Hispanic_Two_Races_SomeOther',\n",
        "    'Not_Hispanic_Two_Races_Exclude_SomeOther',\n",
        "    'Hispanic_White',\n",
        "    'Hispanic_non_White',\n",
        "    'Hispanic_Black_Alone',\n",
        "    'Hispanic_AIAN_Alone',\n",
        "    'Hispanic_Asian_Alone',\n",
        "    'Hispanic_NHPI_Alone',\n",
        "    'Hispanic_Other_Race_Alone',\n",
        "    'Hispanic_Two_or_More_Races',\n",
        "    'Hispanic_Two_Races_SomeOther',\n",
        "    'Hispanic_Two_Races_Exclude_SomeOther',\n",
        "    'STATEFP',\n",
        "    'GEOID',\n",
        "    'GEOIDFQ',\n",
        "    'MTFCC',\n",
        "    'FUNCSTAT',\n",
        "    'DATE_FULL',\n",
        "    'YEAR_left',\n",
        "    'MONTH',\n",
        "    'x',\n",
        "    'y',\n",
        "    'ALAND',\n",
        "    'AWATER',\n",
        "    'INTPTLAT',\n",
        "    'INTPTLON',\n",
        "    'Shape_Leng',\n",
        "    'Shape_Area',\n",
        "    'ORIG_FID',\n",
        "    'YEAR_right',\n",
        "    'STUSAB',\n",
        "    'REGIONA',\n",
        "    'DIVISIONA',\n",
        "    'STATE',\n",
        "    'STATEA',\n",
        "    'COUSUBA',\n",
        "    'PLACEA',\n",
        "    'BLKGRPA',\n",
        "    'CONCITA',\n",
        "    'AIANHHA',\n",
        "    'RES_ONLYA',\n",
        "    'TRUSTA',\n",
        "    'AIHHTLI',\n",
        "    'AITSA',\n",
        "    'ANRCA',\n",
        "    'CBSAA',\n",
        "    'CSAA',\n",
        "    'METDIVA',\n",
        "    'CNECTA',\n",
        "    'NECTADIV',\n",
        "    'UAA',\n",
        "    'CDCURRA',\n",
        "    'SLDUA',\n",
        "    'SLDLA',\n",
        "    'ZCTA5A',\n",
        "    'SUBMCDA',\n",
        "    'SDELMA',\n",
        "    'SDSECA',\n",
        "    'SDUNIA',\n",
        "    'PCI',\n",
        "    'PUMAA',\n",
        "    'GEO_ID',\n",
        "    'BTTRA',\n",
        "    'BTBGA'\n",
        "]\n",
        "semcog_new_2 = semcog_new_2.drop(columns=moe_cols_to_drop_2, errors='ignore')\n",
        "del moe_cols_to_drop_2\n",
        "\n",
        "# Next part I find average crash rates, and most common race, income level, etc. for each row\n",
        "def analyze_intersection(semcog_new_2, road1, road2):\n",
        "\n",
        "    # Get the target intersection data\n",
        "    target_intersection = semcog_new_2[\n",
        "        ((semcog_new_2['INTERROAD'].str.contains(road1, case=False, na=False)) &\n",
        "         (semcog_new_2['MAINROAD'].str.contains(road2, case=False, na=False))) |\n",
        "        ((semcog_new_2['INTERROAD'].str.contains(road2, case=False, na=False)) &\n",
        "         (semcog_new_2['MAINROAD'].str.contains(road1, case=False, na=False)))\n",
        "    ].copy()\n",
        "\n",
        "    def determine_income_level(row):\n",
        "        \"\"\"Calculate mean income level based on household income percentages\"\"\"\n",
        "        income_cols = [\n",
        "            'HH_Income_Less_Than_10K_Percent_values',\n",
        "            'HH_Income_10K_to_14_999_Percent_values',\n",
        "            'HH_Income_15K_to_19_999_Percent_values',\n",
        "            'HH_Income_20K_to_24_999_Percent_values',\n",
        "            'HH_Income_25K_to_29_999_Percent_values',\n",
        "            'HH_Income_30K_to_34_999_Percent_values',\n",
        "            'HH_Income_35K_to_39_999_Percent_values',\n",
        "            'HH_Income_40K_to_44_999_Percent_values',\n",
        "            'HH_Income_45K_to_49_999_Percent_values',\n",
        "            'HH_Income_50K_to_59_999_Percent_values',\n",
        "            'HH_Income_60K_to_74_999_Percent_values',\n",
        "            'HH_Income_75K_to_99_999_Percent_values',\n",
        "            'HH_Income_125K_to_149_999_Percent_values',\n",
        "            'HH_Income_150K_to_199_999_Percent_values',\n",
        "            'HH_Income_200K_or_More_Percent_values'\n",
        "        ]\n",
        "\n",
        "        income_values = []\n",
        "        for col in income_cols:\n",
        "            if col in row.index and pd.notna(row[col]):\n",
        "                income_values.append(row[col])\n",
        "\n",
        "\n",
        "    def get_top_races(row):\n",
        "        \"\"\"Get top 2 most common races\"\"\"\n",
        "        race_cols = ['White', 'Black', 'Asian', 'NHPI', 'Hispanic_or_Latino']\n",
        "        race_values = {}\n",
        "\n",
        "        for col in race_cols:\n",
        "            if col in row.index:\n",
        "                race_values[col] = row[col]\n",
        "\n",
        "        # Sort by values (descending)\n",
        "        sorted_races = sorted(race_values.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top 2\n",
        "        top_2 = [race[0] for race in sorted_races[:2]]\n",
        "        return top_2\n",
        "\n",
        "\n",
        "    def get_top_income_brackets(row):\n",
        "        \"\"\"Get top 3 income brackets\"\"\"\n",
        "        income_cols = [\n",
        "            'HH_Income_Less_Than_10K_Percent_values',\n",
        "            'HH_Income_10K_to_14_999_Percent_values',\n",
        "            'HH_Income_15K_to_19_999_Percent_values',\n",
        "            'HH_Income_20K_to_24_999_Percent_values',\n",
        "            'HH_Income_25K_to_29_999_Percent_values',\n",
        "            'HH_Income_30K_to_34_999_Percent_values',\n",
        "            'HH_Income_35K_to_39_999_Percent_values',\n",
        "            'HH_Income_40K_to_44_999_Percent_values',\n",
        "            'HH_Income_45K_to_49_999_Percent_values',\n",
        "            'HH_Income_50K_to_59_999_Percent_values',\n",
        "            'HH_Income_60K_to_74_999_Percent_values',\n",
        "            'HH_Income_75K_to_99_999_Percent_values',\n",
        "            'HH_Income_125K_to_149_999_Percent_values',\n",
        "            'HH_Income_150K_to_199_999_Percent_values',\n",
        "            'HH_Income_200K_or_More_Percent_values'\n",
        "        ]\n",
        "\n",
        "        income_values = {}\n",
        "        for col in income_cols:\n",
        "            if col in row.index:\n",
        "                income_values[col] = row[col]\n",
        "\n",
        "        # Sort by values (descending)\n",
        "        sorted_income = sorted(income_values.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top 3 and map to readable names\n",
        "        income_mapping = {\n",
        "            'HH_Income_Less_Than_10K_Percent_values': 'Under $10K',\n",
        "            'HH_Income_10K_to_14_999_Percent_values': '$10K-$14,999',\n",
        "            'HH_Income_15K_to_19_999_Percent_values': '$15K-$19,999',\n",
        "            'HH_Income_20K_to_24_999_Percent_values': '$20K-$24,999',\n",
        "            'HH_Income_25K_to_29_999_Percent_values': '$25K-$29,999',\n",
        "            'HH_Income_30K_to_34_999_Percent_values': '$30K-$34,999',\n",
        "            'HH_Income_35K_to_39_999_Percent_values': '$35K-$39,999',\n",
        "            'HH_Income_40K_to_44_999_Percent_values': '$40K-$44,999',\n",
        "            'HH_Income_45K_to_49_999_Percent_values': '$45K-$49,999',\n",
        "            'HH_Income_50K_to_59_999_Percent_values': '$50K-$59,999',\n",
        "            'HH_Income_60K_to_74_999_Percent_values': '$60K-$74,999',\n",
        "            'HH_Income_75K_to_99_999_Percent_values': '$75K-$99,999',\n",
        "            'HH_Income_125K_to_149_999_Percent_values': '$125K-$149,999',\n",
        "            'HH_Income_150K_to_199_999_Percent_values': '$150K-$199,999',\n",
        "            'HH_Income_200K_or_More_Percent_values': '$200K+'\n",
        "        }\n",
        "\n",
        "        top_3 = [income_mapping.get(item[0], item[0]) for item in sorted_income[:3]]\n",
        "        return top_3\n",
        "\n",
        "    def get_poverty_level(row):\n",
        "        \"\"\"Calculate weighted average poverty level and return the corresponding income range\"\"\"\n",
        "        poverty_cols_with_ranges = [\n",
        "            ('Under_50_Percent_Poverty_Percent_values', 'Under 50% of Poverty Level'),\n",
        "            ('50_to_99_Percent_Poverty_Percent_values', '50-99% of Poverty Level'),\n",
        "            ('100_to_124_Percent_Poverty_Percent_values', '100-124% of Poverty Level'),\n",
        "            ('125_to_149_Percent_Poverty_Percent_values', '125-149% of Poverty Level'),\n",
        "            ('150_to_184_Percent_Poverty_Percent_values', '150-184% of Poverty Level'),\n",
        "            ('185_to_199_Percent_Poverty_Percent_values', '185-199% of Poverty Level'),\n",
        "            ('200_Percent_or_More_Poverty_Percent_values', '200%+ of Poverty Level')\n",
        "        ]\n",
        "\n",
        "        # Assign midpoint values to each poverty bracket for weighted average calculation\n",
        "        poverty_midpoints = [25, 74.5, 112, 137, 167, 192, 250]  # Midpoints of each bracket\n",
        "\n",
        "        total_weighted_value = 0\n",
        "        total_percentage = 0\n",
        "\n",
        "        for i, (col, range_name) in enumerate(poverty_cols_with_ranges):\n",
        "            if col in row.index and pd.notna(row[col]) and row[col] > 0:\n",
        "                percentage = row[col]\n",
        "                weighted_value = percentage * poverty_midpoints[i]\n",
        "                total_weighted_value += weighted_value\n",
        "                total_percentage += percentage\n",
        "\n",
        "        if total_percentage == 0:\n",
        "            return \"No poverty data available\"\n",
        "\n",
        "        # Calculate weighted average poverty level percentage\n",
        "        weighted_avg_poverty_level = total_weighted_value / total_percentage\n",
        "\n",
        "        # Map the weighted average back to the appropriate range\n",
        "        if weighted_avg_poverty_level < 50:\n",
        "            return \"Under 50% of Poverty Level\"\n",
        "        elif weighted_avg_poverty_level < 100:\n",
        "            return \"50-99% of Poverty Level\"\n",
        "        elif weighted_avg_poverty_level < 125:\n",
        "            return \"100-124% of Poverty Level\"\n",
        "        elif weighted_avg_poverty_level < 150:\n",
        "            return \"125-149% of Poverty Level\"\n",
        "        elif weighted_avg_poverty_level < 185:\n",
        "            return \"150-184% of Poverty Level\"\n",
        "        elif weighted_avg_poverty_level < 200:\n",
        "            return \"185-199% of Poverty Level\"\n",
        "        else:\n",
        "            return \"200%+ of Poverty Level\"\n",
        "\n",
        "    def analyze_crashes_all_intersection(df):\n",
        "        \"\"\"Analyze crash rates using ALL crashes at the intersection\"\"\"\n",
        "        if len(df) == 0:\n",
        "            return {}\n",
        "\n",
        "        # Crash-related columns for rate calculation\n",
        "        crash_cols = [\n",
        "            'PERCENT_INJURED', 'MOTORIST_INJURY', 'TOTAL_INJURY', 'BIKEINJURY',\n",
        "            'PARTYCOUNT', 'SPEEDING', 'DRIVEWAY', 'WORK_ZONE', 'DISTRACTED',\n",
        "            'EMERGENCY', 'COMMERCIAL', 'TRAIN', 'MOTORCYCLE', 'BICYCLE',\n",
        "            'PEDESTRIAN', 'DRUG', 'ALCOHOL', 'HITNRUN', 'SCHOOLBUS', 'DEER'\n",
        "        ]\n",
        "\n",
        "        # Additional columns for averages (not rates)\n",
        "        avg_cols = ['SPEEDLIMIT', 'ROADLANES']\n",
        "\n",
        "        # Maximum AADTR (busier road determines intersection risk)\n",
        "        aadtr_values = df['AADTR'][df['AADTR'] > 0]  # Exclude zero values\n",
        "        if len(aadtr_values) == 0:\n",
        "            return {}\n",
        "\n",
        "        max_aadtr = aadtr_values.max()\n",
        "\n",
        "        # Calculate crash statistics using ALL crashes at the intersection\n",
        "        stats = {}\n",
        "\n",
        "        # Calculate crash rates per AADTR for each crash type\n",
        "        for col in crash_cols:\n",
        "            if col in df.columns:\n",
        "                if col in ['PERCENT_INJURED', 'PARTYCOUNT']:\n",
        "                    # For these, calculate average value across all crashes\n",
        "                    stats[f'avg_{col}'] = df[col].mean()\n",
        "                else:\n",
        "                    # For binary columns, calculate rate = (sum of crashes with this factor) / max_AADTR\n",
        "                    crash_count = df[col].sum()\n",
        "                    stats[f'rate_{col}'] = (crash_count / max_aadtr) * 1000  # Rate per 1000 AADTR\n",
        "\n",
        "        # Calculate total crash rate for this intersection\n",
        "        total_crashes = len(df)\n",
        "        stats['rate_TOTAL_CRASHES'] = (total_crashes / max_aadtr) * 1000  # Rate per 1000 AADTR\n",
        "        stats['total_crashes_at_intersection'] = total_crashes  # Raw count of all crashes at intersection\n",
        "\n",
        "        # Calculate crash rate for crashes with NONE of the specific factors\n",
        "        factor_cols = [col for col in crash_cols if col not in ['PERCENT_INJURED', 'PARTYCOUNT']]\n",
        "\n",
        "        # Find crashes that have none of these factors (all zeros)\n",
        "        crashes_with_no_factors = df[factor_cols].sum(axis=1) == 0\n",
        "        crashes_no_factors_count = crashes_with_no_factors.sum()\n",
        "        stats['rate_NO_FACTORS'] = (crashes_no_factors_count / max_aadtr) * 1000  # Rate per 1000 AADTR\n",
        "\n",
        "        # Add averages for non-rate columns\n",
        "        for col in avg_cols:\n",
        "            if col in df.columns:\n",
        "                stats[f'avg_{col}'] = df[col].mean()\n",
        "\n",
        "        # Add the AADTR used for calculations\n",
        "        stats['AADTR_used'] = max_aadtr\n",
        "\n",
        "        return stats\n",
        "\n",
        "    results = []\n",
        "\n",
        "    if len(target_intersection) > 0:\n",
        "        # For the target intersection, use ALL crashes regardless of AADTR\n",
        "        # But still get most common track and NFC for finding similar intersections\n",
        "        most_common_track = target_intersection['COUNTY'].mode().iloc[0] if len(target_intersection['COUNTY'].mode()) > 0 else target_intersection['COUNTY'].iloc[0]\n",
        "        most_common_nfc = target_intersection['NFC'].mode().iloc[0] if len(target_intersection['NFC'].mode()) > 0 else target_intersection['NFC'].iloc[0]\n",
        "\n",
        "        # Skip if all AADTR values are 0\n",
        "        valid_aadtr = target_intersection['AADTR'][target_intersection['AADTR'] > 0]\n",
        "        if len(valid_aadtr) == 0:\n",
        "            return results\n",
        "\n",
        "        # Target intersection analysis using ALL crashes\n",
        "        target_avg = target_intersection.mean(numeric_only=True)\n",
        "        target_analysis = {\n",
        "            'Location': f'{road1}-{road2}',\n",
        "            'AADTR': valid_aadtr.max(),  # Use the max AADTR from the intersection\n",
        "            'NFC': most_common_nfc,\n",
        "            'Top_2_Races': get_top_races(target_avg),\n",
        "            'Top_3_Income_Brackets': get_top_income_brackets(target_avg),\n",
        "            'Mean_Poverty_Level': get_poverty_level(target_avg),\n",
        "            'Median_HH_Income': target_avg.get('Median_HH_Income_2023', 0),\n",
        "            'Total_Crashes': len(target_intersection)\n",
        "        }\n",
        "\n",
        "        # Add crash statistics using ALL crashes at intersection\n",
        "        crash_stats = analyze_crashes_all_intersection(target_intersection)\n",
        "        target_analysis.update(crash_stats)\n",
        "        results.append(target_analysis)\n",
        "\n",
        "        # Find similar intersections by NFC, AADTR, and tract\n",
        "        # Use the most common AADTR from the target intersection for matching\n",
        "        most_common_aadtr = target_intersection['AADTR'].mode().iloc[0] if len(target_intersection['AADTR'].mode()) > 0 else target_intersection['AADTR'].iloc[0]\n",
        "\n",
        "        similar_intersections = semcog_new_2[\n",
        "            (semcog_new_2['COUNTY'] == most_common_track) &\n",
        "            (semcog_new_2['NFC'] == most_common_nfc) &\n",
        "            (semcog_new_2['AADTR'] == most_common_aadtr)\n",
        "        ].copy()\n",
        "\n",
        "        # Remove the target intersection from similar intersections\n",
        "        target_mask = (\n",
        "            ((similar_intersections['INTERROAD'].str.contains(road1, case=False, na=False)) &\n",
        "             (similar_intersections['MAINROAD'].str.contains(road2, case=False, na=False))) |\n",
        "            ((similar_intersections['INTERROAD'].str.contains(road2, case=False, na=False)) &\n",
        "             (similar_intersections['MAINROAD'].str.contains(road1, case=False, na=False)))\n",
        "        )\n",
        "\n",
        "        rest_of_data = similar_intersections[~target_mask]\n",
        "\n",
        "        # Similar intersections analysis\n",
        "        if len(rest_of_data) > 0:\n",
        "            rest_avg = rest_of_data.mean(numeric_only=True)\n",
        "            rest_analysis = {\n",
        "                'Location': f'Similar to {road1}-{road2}',\n",
        "                'AADTR': most_common_aadtr,\n",
        "                'NFC': most_common_nfc,\n",
        "                'Top_2_Races': get_top_races(rest_avg),\n",
        "                'Top_3_Income_Brackets': get_top_income_brackets(rest_avg),\n",
        "                'Mean_Poverty_Level': get_poverty_level(rest_avg),\n",
        "                'Median_HH_Income': rest_avg.get('Median_HH_Income_2023', 0),\n",
        "                'Total_Crashes': len(rest_of_data)\n",
        "            }\n",
        "\n",
        "            # Add crash statistics for similar intersections\n",
        "            crash_stats = analyze_crashes_all_intersection(rest_of_data)\n",
        "            rest_analysis.update(crash_stats)\n",
        "            results.append(rest_analysis)\n",
        "\n",
        "    return results\n",
        "all_results = pd.DataFrame()\n",
        "\n",
        "# Analyze different intersections\n",
        "intersections_to_analyze = [\n",
        "    ('Hartland', 'Rovey'),\n",
        "    ('Tienken', 'Sheldon'),\n",
        "    ('Utica', 'Dodge'),\n",
        "    ('Lee', 'SB'),\n",
        "    ('Lee', 'Whitmore'),\n",
        "    ('25', 'Hayes'),\n",
        "    ('Maple', 'Drake'),\n",
        "    ('Maple', 'Farmington'),\n",
        "    ('BogieLake', 'CooleyLake'),\n",
        "    ('Main', 'S3'),\n",
        "    ('Loop', 'Commerce'),\n",
        "    ('Cooley', 'Oxbow'),\n",
        "    ('14', 'Farmington'),\n",
        "    ('Martin', 'Library'),\n",
        "    ('Martin', 'PGA'),\n",
        "    ('Martin', 'Oakley'),\n",
        "    ('14', 'Orchard'),\n",
        "    ('Grand', 'Hudson'),\n",
        "    ('Grand', 'Lyon'),\n",
        "    ('Hudson', 'Pontiac'),\n",
        "    ('White', 'Duck'),\n",
        "    ('Hamlin', 'Livernois'),\n",
        "    ('Tienken', 'Livernois'),\n",
        "    ('Kercheval', 'Wayburn'),\n",
        "    ('Ryder', 'Everett'),\n",
        "    ('Ryder', 'Connors'),\n",
        "    ('Firewood', 'Raintree'),\n",
        "    ('Coachwood', 'Falcon'),\n",
        "    ('Pioneer', 'Library'),\n",
        "    ('Anthony', 'NB'),\n",
        "    ('Anthony', 'SB'),\n",
        "    ('Chilson', 'CoonLake'),\n",
        "    ('Napier', '10'),\n",
        "    ('Taft', 'Morgan'),\n",
        "    ('Mile', 'Romeo'),\n",
        "    ('19', 'Romeo'),\n",
        "    ('Romeo', 'Cass'),\n",
        "    ('Romeo', 'Canal'),\n",
        "    ('Baldwin', 'Gregory'),\n",
        "    ('Baldwin', 'Judah'),\n",
        "    ('Hamburg', 'Winans'),\n",
        "    ('Crescent', 'Town'),\n",
        "    ('Village', 'Green'),\n",
        "    ('Durant', 'Chayne'),\n",
        "    ('Cole', 'Chevrolet'),\n",
        "    ('Huron', 'Nixon'),\n",
        "    ('Scio', 'Wagner'),\n",
        "    ('Baker', 'DanHoey'),\n",
        "    ('Bemis', 'Moon'),\n",
        "    ('Whittaker', 'Merrit'),\n",
        "    ('Textile', 'Hitchingham'),\n",
        "    ('Textile', 'StonyCreek'),\n",
        "    ('Whittaker', 'StonyCreek'),\n",
        "    ('Geddes', 'Ridge'),\n",
        "    ('Geddes', 'Superior'),\n",
        "    ('Campus', 'Community'),\n",
        "    ('Maple', 'WB'),\n",
        "    ('Maple', 'EB'),\n",
        "    ('Geddes', 'NB'),\n",
        "    ('Geddes', 'SB'),\n",
        "    ('Geddes', 'Earhart'),\n",
        "    ('8', 'SB'),\n",
        "    ('Territorial', 'NB'),\n",
        "    ('Territorial', 'SB'),\n",
        "    ('8Mile', 'Whitmore'),\n",
        "    ('Franklin', '11Mile'),\n",
        "    ('Yellowstone', 'Johnson'),\n",
        "    ('Carriage', 'Glacier'),\n",
        "    ('Pioneer', 'Meadow'),\n",
        "    ('Kensington', 'Jacoby'),\n",
        "    ('Bell', 'Coventry'),\n",
        "    ('Metro', 'Sail'),\n",
        "    ('33', 'McKay')\n",
        "]\n",
        "\n",
        "# Run analysis for each intersection and append results\n",
        "for road1, road2 in intersections_to_analyze:\n",
        "    intersection_results = analyze_intersection(semcog_new_2, road1, road2)\n",
        "    if intersection_results:  # Only append if results found\n",
        "        intersection_df = pd.DataFrame(intersection_results)\n",
        "        all_results = pd.concat([all_results, intersection_df], ignore_index=True)\n",
        "\n",
        "all_locations = set(all_results['Location'])\n",
        "\n",
        "# Find which intersections have a \"Similar to\" counterpart\n",
        "intersections_with_similar = set()\n",
        "\n",
        "for location in all_locations:\n",
        "    if not location.startswith('Similar to '):\n",
        "        # Check if there's a corresponding \"Similar to\" entry\n",
        "        similar_location = f'Similar to {location}'\n",
        "        if similar_location in all_locations:\n",
        "            intersections_with_similar.add(location)\n",
        "            intersections_with_similar.add(similar_location)\n",
        "\n",
        "# Filter the results to only include intersections that have both original and similar\n",
        "filtered_results = all_results[all_results['Location'].isin(intersections_with_similar)].copy()\n",
        "\n",
        "# Sort so that each intersection and its \"Similar to\" are grouped together\n",
        "filtered_results = filtered_results.sort_values('Location')\n",
        "all_results = filtered_results\n",
        "\n",
        "del road1, road2, intersections_to_analyze, intersection_results, intersection_df, col, semcog_new, semcog_new_2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "def plot_total_injury_by_individual_income(df, title, color='steelblue'):\n",
        "    \"\"\"Plot total injury rates with individual columns for each household income level\"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Remove any rows with missing income data\n",
        "    df_copy = df_copy.dropna(subset=['Median_HH_Income', 'rate_TOTAL_INJURY'])\n",
        "\n",
        "    # Group by exact median household income and get the rate_TOTAL_INJURY\n",
        "    # Use first() to get the rate for each income level (assuming one intersection per income level)\n",
        "    grouped = df_copy.groupby('Median_HH_Income')['rate_TOTAL_INJURY'].first().sort_index()\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
        "\n",
        "    # Create bar chart with individual income levels\n",
        "    x_positions = range(len(grouped))\n",
        "    bars = ax.bar(x_positions, grouped.values, color=color,\n",
        "                  edgecolor='white', linewidth=1.5, alpha=0.8)\n",
        "\n",
        "    # Set title and labels\n",
        "    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Median Household Income', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Total Injury Rate per 1000 AADTR', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Format x-axis labels to show income values\n",
        "    income_labels = [f'${int(income/1000)}K' for income in grouped.index]\n",
        "    ax.set_xticks(x_positions)\n",
        "    ax.set_xticklabels(income_labels, rotation=45, ha='right', fontsize=10)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for i, (bar, value) in enumerate(zip(bars, grouped.values)):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(grouped.values)*0.01,\n",
        "               f'{value:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # Add grid for better readability\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "    ax.set_axisbelow(True)\n",
        "\n",
        "    # Improve layout\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"Number of income levels: {len(grouped)}\")\n",
        "    print(f\"Income range: ${int(grouped.index.min()/1000)}K - ${int(grouped.index.max()/1000)}K\")\n",
        "    print(f\"Total injury rate range: {grouped.min():.3f} - {grouped.max():.3f}\")\n",
        "    print(f\"Mean total injury rate: {grouped.mean():.3f}\")\n",
        "    print(f\"Median total injury rate: {grouped.median():.3f}\")\n",
        "\n",
        "    return grouped\n",
        "\n",
        "def plot_total_injury_scatter_safe(df, title, color='steelblue'):\n",
        "    \"\"\"Plot total injury rates as scatter plot vs income with safe trend line calculation\"\"\"\n",
        "    df_copy = df.copy().dropna(subset=['Median_HH_Income', 'rate_TOTAL_INJURY'])\n",
        "    df_copy = df_copy.sort_values('Median_HH_Income')\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "    # Create scatter plot\n",
        "    scatter = ax.scatter(df_copy['Median_HH_Income'], df_copy['rate_TOTAL_INJURY'],\n",
        "                        c=color, s=100, alpha=0.7, edgecolors='white', linewidth=1.5)\n",
        "\n",
        "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Median Household Income ($)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Total Injury Rate per 1000 AADTR', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Format x-axis to show income in thousands\n",
        "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
        "\n",
        "    # Try to add trend line with error handling\n",
        "    try:\n",
        "        # Check if we have enough unique points and no constant values\n",
        "        if len(df_copy) > 2 and df_copy['rate_TOTAL_INJURY'].std() > 1e-10:\n",
        "            z = np.polyfit(df_copy['Median_HH_Income'], df_copy['rate_TOTAL_INJURY'], 1)\n",
        "            p = np.poly1d(z)\n",
        "            ax.plot(df_copy['Median_HH_Income'], p(df_copy['Median_HH_Income']),\n",
        "                   \"r--\", alpha=0.8, linewidth=2, label='Trend Line')\n",
        "\n",
        "            # Calculate correlation\n",
        "            correlation = np.corrcoef(df_copy['Median_HH_Income'], df_copy['rate_TOTAL_INJURY'])[0,1]\n",
        "            ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=ax.transAxes,\n",
        "                   fontsize=11, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "            ax.legend(fontsize=10)\n",
        "        else:\n",
        "            print(\"Warning: Cannot compute trend line - insufficient variation in data\")\n",
        "    except (np.linalg.LinAlgError, ValueError) as e:\n",
        "        print(f\"Warning: Could not compute trend line due to numerical issues: {e}\")\n",
        "\n",
        "    ax.grid(True, linestyle='--', alpha=0.4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Separate unique and similar intersections from all_results\n",
        "unique_intersections = all_results[~all_results['Location'].str.contains('Similar to', na=False)].copy()\n",
        "similar_intersections = all_results[all_results['Location'].str.contains('Similar to', na=False)].copy()\n",
        "\n",
        "print(\"=== TOTAL INJURY RATES BY INDIVIDUAL INCOME LEVELS ===\")\n",
        "\n",
        "# For unique intersections (roundabouts)\n",
        "print(\"\\n=== ROUNDABOUTS TOTAL INJURY ANALYSIS ===\")\n",
        "print(f\"Number of unique intersections: {len(unique_intersections)}\")\n",
        "\n",
        "if len(unique_intersections) > 0:\n",
        "    print(\"\\n1. Total Injury Rates by Individual Income Levels:\")\n",
        "    unique_injury_data = plot_total_injury_by_individual_income(\n",
        "        unique_intersections,\n",
        "        'Total Injury Rates by Individual Median Household Income Levels (Roundabouts)',\n",
        "        'steelblue'\n",
        "    )\n",
        "\n",
        "    print(\"\\n2. Total Injury Rates vs Median Household Income (Scatter):\")\n",
        "    plot_total_injury_scatter_safe(\n",
        "        unique_intersections,\n",
        "        'Total Injury Rates vs Median Household Income (Roundabouts)',\n",
        "        'steelblue'\n",
        "    )\n",
        "else:\n",
        "    print(\"No unique intersections found in data\")\n",
        "\n",
        "# For similar intersections\n",
        "print(\"\\n=== SIMILAR INTERSECTIONS TOTAL INJURY ANALYSIS ===\")\n",
        "print(f\"Number of similar intersections: {len(similar_intersections)}\")\n",
        "\n",
        "if len(similar_intersections) > 0:\n",
        "    print(\"\\n1. Total Injury Rates by Individual Income Levels:\")\n",
        "    similar_injury_data = plot_total_injury_by_individual_income(\n",
        "        similar_intersections,\n",
        "        'Total Injury Rates by Individual Median Household Income Levels (Similar Intersections)',\n",
        "        'darkorange'\n",
        "    )\n",
        "\n",
        "    print(\"\\n2. Total Injury Rates vs Median Household Income (Scatter):\")\n",
        "    plot_total_injury_scatter_safe(\n",
        "        similar_intersections,\n",
        "        'Total Injury Rates vs Median Household Income (Similar Intersections)',\n",
        "        'darkorange'\n",
        "    )\n",
        "else:\n",
        "    print(\"No similar intersections found in data\")\n",
        "\n",
        "# All intersections combined analysis\n",
        "print(\"\\n=== ALL INTERSECTIONS TOTAL INJURY ANALYSIS ===\")\n",
        "print(f\"Number of all intersections: {len(all_results)}\")\n",
        "\n",
        "if len(all_results) > 0:\n",
        "    print(\"\\n1. Total Injury Rates by Individual Income Levels:\")\n",
        "    all_injury_data = plot_total_injury_by_individual_income(\n",
        "        all_results,\n",
        "        'Total Injury Rates by Individual Median Household Income Levels (All Intersections)',\n",
        "        'darkgreen'\n",
        "    )\n",
        "\n",
        "    print(\"\\n2. Total Injury Rates vs Median Household Income (Scatter):\")\n",
        "    plot_total_injury_scatter_safe(\n",
        "        all_results,\n",
        "        'Total Injury Rates vs Median Household Income (All Intersections)',\n",
        "        'darkgreen'\n",
        "    )\n",
        "else:\n",
        "    print(\"No intersection data found in all_results\")\n"
      ]
    }
  ]
}